{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10642509,"sourceType":"datasetVersion","datasetId":6589509}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:12.042410Z","iopub.execute_input":"2025-02-05T22:16:12.042899Z","iopub.status.idle":"2025-02-05T22:16:12.494599Z","shell.execute_reply.started":"2025-02-05T22:16:12.042852Z","shell.execute_reply":"2025-02-05T22:16:12.493238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### All code blocks are either functions or classses so that its easier to debug\n## Bad code is here","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:12.496104Z","iopub.execute_input":"2025-02-05T22:16:12.496551Z","iopub.status.idle":"2025-02-05T22:16:12.501434Z","shell.execute_reply.started":"2025-02-05T22:16:12.496524Z","shell.execute_reply":"2025-02-05T22:16:12.499872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n## all imports\nimport dask\nimport pyarrow.parquet as pq\nimport dask.dataframe as dd\nimport os\nimport shutil\nimport json\nfrom enum import Enum\nfrom datetime import datetime\nfrom ydata_profiling import ProfileReport\nfrom pathlib import Path\nimport random\n\n\nfrom dask.distributed import LocalCluster\n##Metrics\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.metrics import mutual_info_score, adjusted_rand_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n\n\n#For excel stuff\nimport openpyxl\nfrom openpyxl.drawing.image import Image\n\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom shapely.geometry import Point\n\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom pmdarima.arima.utils import ndiffs, nsdiffs\nimport numpy as np\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error,mean_squared_error\nfrom pmdarima.metrics import smape\nfrom pmdarima import model_selection\nfrom neuralprophet import NeuralProphet, set_log_level\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose, STL, MSTL\nimport plotly.express as px\nfrom prophet import Prophet\n\n\n\n#from plotly_resampler import register_plotly_resampler\n#register_plotly_resampler(mode='auto')\n\n\n##h3\nimport h3\n\n# Call the register function once and all Figures/FigureWidgets will be wrapped\n# according to the register_plotly_resampler its `mode` argument\n#register_plotly_resampler(mode='auto')\n\n# Concurrency\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nimport time\n\n#  Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random state\nrandom_state = 42\n# Set figure size\nplt.rcParams[\"figure.figsize\"] = (20, 20)\n\n\nparquet_file = {\n    'merged':'/kaggle/input/road-accident-parquet/merged.parquet',\n    2019: '/kaggle/input/road-accident-parquet/2019.parquet',\n    2020: '/kaggle/input/road-accident-parquet/2020.parquet',\n    2021: '/kaggle/input/road-accident-parquet/2021.parquet',\n    2022: '/kaggle/input/road-accident-parquet/2022.parquet',\n    2023: '/kaggle/input/road-accident-parquet/2023.parquet'\n}\n\n\nfrom dask.distributed import Client\nclient = Client()\n\npd.set_option('display.max_columns', None)\n\n\n\nclass ExtensionMethods:\n    @staticmethod\n    def generate_filename(filename=None,extension=None):\n        current_datetime = datetime.now()\n        f = current_datetime.strftime(\"%Y_%m_%d_%H%M\")\n        if (filename is None) or (extension is None):\n            return str(f)\n        else:\n            stitched_f = str(filename)+\"_\"+str(f)+\".\"+str(extension)\n            return str(stitched_f)\n\n    @staticmethod\n    def get_file_name_without_extension(filename):\n        if filename == None:\n            return \"Provide a file\"\n        return Path(filename).stem\n\n\ndef create_parquet(data, filename='test'):\n    if data is None:\n        raise ValueError(\"Data can't be None for Parquet Creation\")\n    obj_cols = data.select_dtypes(include =['object','category'], exclude=['datetime','datetime64','datetimetz']).columns\n    for col in obj_cols:\n        data[col]=data[col].astype(str)\n    filepath = os.path.join(os.getcwd(),ExtensionMethods.generate_filename(f\"{filename}\", \"parquet\"))\n    data.to_parquet(filepath, engine='pyarrow',compression=\"zstd\", compression_level=10, index=False)\n    print(f\"\\n Finished Saving parquet to: {filepath}\")\n\n\n\n\nclass CategoryBaseEnum(Enum):\n    @classmethod\n    def Name(cls):\n        return f\"{cls.__name__}\"\n    @classmethod\n    def IsCategory(cls):\n        return True if len(cls.__members__) > 0 else False\n    @classmethod\n    def get_description(cls):\n        return cls.__doc__ or \"No description available\"\n    @classmethod\n    def to_dict(cls):\n        return {member.name:member.value for member in cls}\n    @classmethod\n    def to_json(cls, indent=4):\n        return json.dumps(cls.to_dict(), indent=indent)\n    @classmethod\n    def enum_keys(cls):\n        return [member.name for member in cls]\n\n\nclass FileTypeEnum(Enum):\n    @classmethod\n    def Name(cls):\n        return f\"{cls.__name__}\"\n    @classmethod\n    def get_description(cls):\n        return cls.__doc__ or \"No description available\"\n    @classmethod\n    def to_dict(cls):\n        return {member.name:member.value.to_dict() for member in cls}\n    @classmethod\n    def enum_keys(cls):\n        return [member.name for member in cls]\n    @classmethod\n    def IsCategory(cls):\n        return {member.name:member.value.IsCategory() for member in cls}\n    @classmethod\n    def MegaDictionary(cls):\n        return {member.name:[member.value.to_dict(),member.value.IsCategory()] for member in cls}\n\n\nclass RoadAccidentEnum(FileTypeEnum):\n    '''Collection of all the categories for file type 'vehicle.csv' '''\n\n    class accident_id(CategoryBaseEnum):\n        '''The Index/Number of the Crash follows the pattern yyyyxxxxx and is the index column'''\n\n    class vehicle_id(CategoryBaseEnum):\n        '''The vehicle id  in terms of xxx-xxx'''\n\n    class vehicle_category(CategoryBaseEnum):  # catv Vehicle Category\n        '''The Category of Vehicle involved in the crash'''\n        UNKNOWN = 0\n        LIGHT = 1\n        MEDIUM = 2\n        HEAVY = 3\n        MISC = 4\n\n    class obstacle_static(CategoryBaseEnum):  # obs Static ObstacleHit\n        ''' The Static/Stationary Obstacle Hit'''\n        UNKNOWN = 0\n        NO_OBSTACLE = 1\n        LIGHT = 2\n        MEDIUM = 3\n        HEAVY = 4\n\n    class obstacle_mobile(CategoryBaseEnum):  # obsm #Mobile obstacle hit\n        '''The Dynamic Obstacle Hit'''\n        UNKNOWN = 0\n        ANIMAL_OR_OTHER = 1\n        VEHICLE = 2\n        PEDESTRIAN = 3\n\n    class impact_point(CategoryBaseEnum):  # choc Initial Point of Impact\n        '''The Initial Point of Impact of the crash'''\n        UNKNOWN = 0\n        FRONT_IMPACT = 1\n        SIDE_IMPACT = 2\n        REAR_IMPACT = 3\n        MULTIPLE_IMPACT = 4\n\n    class action(CategoryBaseEnum):  # manv , Main action before crash\n        '''The Main Action performed by the user before the crash'''\n        UNKNOWN = 0\n        STATIC = 1\n        NORMAL_RISK = 2\n        MEDIUM_RISK = 3\n        HIGH_RISK = 4\n\n    class motor(CategoryBaseEnum):  # motor\n        '''The Type of Motor(In terms of fuel type) involved in the crash'''\n        UNKNOWN = 0\n        OTHER = 1\n        TRADITIONAL = 2\n        ELECTRIC_HYBRID = 3\n        NON_MOTORIZED = 4\n\n    class road(CategoryBaseEnum):\n        '''Category of the road'''\n        UNKNOWN = 0\n        OTHER = 1\n        URBAN_ROAD = 2\n        HIGHWAY = 3\n\n    class road_surface(CategoryBaseEnum):\n        '''Surface condition'''\n        UNKNOWN = 0\n        NORMAL = 1\n        MODERATE = 2\n        SEVERE = 3\n\n    class speed_limit(CategoryBaseEnum):\n        '''Maximum speed permitted at the location and time of the accident.'''\n\n    class user_category(CategoryBaseEnum):  #\n        '''User category'''\n        UNKNOWN = 0\n        DRIVER = 1\n        PASSENGER = 2\n        PEDESTRIAN = 3\n\n    class severity(CategoryBaseEnum):\n        ''' Severity of the accident: The injured users are classified into three categories of victims plus the uninjured'''\n        UNKNOWN = 0\n        NO_INJURY = 1\n        MINOR_INJURY = 2\n        MAJOR_INJURY = 3\n        KILLED = 4\n\n    class sex(CategoryBaseEnum):\n        '''User's gender'''\n        UNKNOWN = 0\n        MALE = 1\n        FEMALE = 2\n\n    class dob(CategoryBaseEnum):\n        '''Year of birth of the user'''\n\n    class safety_equipment(CategoryBaseEnum):\n        '''The existence of a safety equipment'''\n        UNKNOWN = 0\n        NORMAL = 1\n        GOOD = 2\n        BEST = 3\n\n    class datetime(CategoryBaseEnum):\n        '''Date Time of the accident'''\n\n    class lum(CategoryBaseEnum):\n        '''Lighting conditions during the accident'''\n        UNKNOWN = 0\n        DAYLIGHT = 1\n        NIGHT_LIT = 2\n        LOW_LIGHT = 3\n        NIGHT_DARK = 4\n\n    class weather(CategoryBaseEnum):\n        '''Weather conditions during the accident'''\n        UNKNOWN = 0\n        LIGHT = 1\n        MEDIUM = 2\n        SEVERE = 3\n\n    class collision_type(CategoryBaseEnum):\n        '''Type of collision'''\n        UNKNOWN = 0\n        NO_COLLISION = 1\n        SIMPLE = 2\n        COMPLEX = 3\n\n    class lat(CategoryBaseEnum):\n        '''Latitude of the accident location'''\n\n    class long(CategoryBaseEnum):\n        '''Longitude of the accident location'''\n\n    class h3(CategoryBaseEnum):\n        '''H3 Index of the accident'''\n\n    class age(CategoryBaseEnum):\n        '''Age of the person at time of accident'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:12.503691Z","iopub.execute_input":"2025-02-05T22:16:12.504201Z","iopub.status.idle":"2025-02-05T22:16:41.347580Z","shell.execute_reply.started":"2025-02-05T22:16:12.504150Z","shell.execute_reply":"2025-02-05T22:16:41.345886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## get \ndef step1():\n    valid_columns = RoadAccidentEnum.to_dict()\n    valid_columns = valid_columns.keys()\n    print(valid_columns)\n\n    data = pd.read_parquet(parquet_file['merged'])\n\n    data = data[[col for col in data.columns if col in valid_columns]]\n\n    data['accident_hex_count'] = data.groupby('h3')['h3'].transform('count')\n    data['date'] = pd.to_datetime(data['datetime']).dt.date\n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.349225Z","iopub.execute_input":"2025-02-05T22:16:41.349566Z","iopub.status.idle":"2025-02-05T22:16:41.357095Z","shell.execute_reply.started":"2025-02-05T22:16:41.349538Z","shell.execute_reply":"2025-02-05T22:16:41.355686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step2(data):\n    duplicates = data[data.duplicated()]\n    num_duplicates = duplicates.shape[0]\n    print(\"Duplicate Check:\")\n    print(f\"Number of duplicate rows: {num_duplicates}\")\n    if num_duplicates > 0:\n        print(\"\\nDuplicate Rows:\")\n        print(duplicates.head())\n        print(\"\\nDropping Duplicate Rows:\")\n        data = data.drop_duplicates()\n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.358998Z","iopub.execute_input":"2025-02-05T22:16:41.359518Z","iopub.status.idle":"2025-02-05T22:16:41.382725Z","shell.execute_reply.started":"2025-02-05T22:16:41.359466Z","shell.execute_reply":"2025-02-05T22:16:41.381219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step3(data):\n    a = RoadAccidentEnum.MegaDictionary()\n    replacement_dict={}\n    for index, value in a.items():\n        if a[index][1]==True:\n            replacement_dict[index]=value[0]\n    data = data.replace(replacement_dict).fillna(0)\n    for index, value in replacement_dict.items():\n        data[index] = pd.to_numeric(data[index])\n        data[index] = data[index].replace(-1,0)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.383982Z","iopub.execute_input":"2025-02-05T22:16:41.384412Z","iopub.status.idle":"2025-02-05T22:16:41.404498Z","shell.execute_reply.started":"2025-02-05T22:16:41.384366Z","shell.execute_reply":"2025-02-05T22:16:41.402930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step4(data):\n    columns_to_keep = [\n    'impact_point', 'road_surface', 'speed_limit', 'user_category', \n    'severity', 'safety_equipment', 'lum', 'weather', \n    'collision_type', 'h3', 'accident_id', 'date','accident_hex_count']\n    filtered_columns = [col for col in columns_to_keep if col in data.columns]\n    aggregated_data = (\n    data[filtered_columns]\n    .groupby('date')\n    .agg({\n        'accident_id': 'count',\n        'road_surface': 'max',\n        'speed_limit': 'max',\n        'lum': 'mean',\n        'weather': 'max',\n        'collision_type': 'max',\n        'accident_hex_count':'mean'\n    })\n    .rename(columns={'accident_id': 'accident_count'})\n    .reset_index())\n    df = aggregated_data.copy()\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.405672Z","iopub.execute_input":"2025-02-05T22:16:41.406032Z","iopub.status.idle":"2025-02-05T22:16:41.423011Z","shell.execute_reply.started":"2025-02-05T22:16:41.405990Z","shell.execute_reply":"2025-02-05T22:16:41.421739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step5(data):\n    ##rename\n    df = data\n    df = df.rename(columns={'accident_count': 'y', 'date': 'ds'})\n    df['ds'] = pd.to_datetime(df['ds'], format='%Y-%m-%d', errors='coerce')\n\n\n    ## Seasonality\n    df[\"summer\"] = 0\n    df.loc[df[\"ds\"].dt.month.isin([6, 7, 8]), \"summer\"] = 1\n    df[\"winter\"] = 0\n    df.loc[df[\"ds\"].dt.month.isin([12, 1, 2]), \"winter\"] = 1\n    df[\"fall\"] = 0\n    df.loc[df[\"ds\"].dt.month.isin([9, 10, 11]), \"fall\"] = 1\n    df[\"spring\"] = 0\n    df.loc[df[\"ds\"].dt.month.isin([3, 4, 5]), \"spring\"] = 1\n\n    df['weekend'] = 0\n    df.loc[df['ds'].dt.dayofweek.isin([5, 6]), \"weekend\"] = 1\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.426404Z","iopub.execute_input":"2025-02-05T22:16:41.427746Z","iopub.status.idle":"2025-02-05T22:16:41.446179Z","shell.execute_reply.started":"2025-02-05T22:16:41.427701Z","shell.execute_reply":"2025-02-05T22:16:41.444934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step6(df):\n    end_date = pd.Timestamp('2023-06-30')\n    mask = df['ds'] <= end_date\n    train = df[mask]\n    val = df[~mask]\n    return train, val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.448436Z","iopub.execute_input":"2025-02-05T22:16:41.448790Z","iopub.status.idle":"2025-02-05T22:16:41.469708Z","shell.execute_reply.started":"2025-02-05T22:16:41.448764Z","shell.execute_reply":"2025-02-05T22:16:41.467588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://medium.com/@roberto98russo/my-favorite-trend-seasonality-decomposition-algorithms-for-time-series-analysis-7fefabd717e5\ndef plot_components(result,what):\n    df = pd.concat([result.observed, result.trend, result.seasonal, result.resid], axis=1)\n    df = df.rename(columns={0: 'Original Data', 'season': 'seasonal', 'observed': 'Original Data'})\n    \n    df_long = df.stack().reset_index()\n    df_long.columns = ['date', 'component', 'value']\n    \n    fig = px.line(df_long, \n                  x='date', \n                  y='value',\n                  color='component',\n                  facet_row='component',\n                  facet_row_spacing=0.05,\n                  height=1000,  \n                  width=1000)   \n    \n    fig.update_layout(\n        title=f'Time Series Decomposition-{what}',\n        xaxis_title='Time',\n        showlegend=True\n    )\n    \n    fig.write_html(ExtensionMethods.generate_filename(f\"Seasonal_Decompose_for_{what}\",'html')) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.470832Z","iopub.execute_input":"2025-02-05T22:16:41.471174Z","iopub.status.idle":"2025-02-05T22:16:41.483653Z","shell.execute_reply.started":"2025-02-05T22:16:41.471145Z","shell.execute_reply":"2025-02-05T22:16:41.482360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##https://medium.com/towards-data-science/finding-seasonal-trends-in-time-series-data-with-python-ce10c37aa861\ndef time_series_analyser(df):\n    ##TODO: Dont need it for the pipeline . Add later to autocreate graphs and pngs\n    data = df[['ds', 'y']]\n    data = data.set_index('ds')\n\n    ##Perform stationary check\n    result = adfuller(data['y'], autolag='AIC')\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    print(f\"  - Stationary: {'Yes' if result[1] < 0.05 else 'No'}\")\n\n    ## perform seasonal decompose and save png\n    decompose = seasonal_decompose(data, model='additive')\n    fig= decompose.plot()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Seasonal Decomposition_Additive\",'png'))  \n    plt.close(fig)\n    print(f\"The Variance for the additive model is {float(decompose.resid.var())}\")\n    \n    decompose = seasonal_decompose(data, model='multiplicative')\n    fig= decompose.plot()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Seasonal Decomposition_Multiplicative\",'png'))  \n    plt.close(fig)\n    print(f\"The Variance for the multiplicative model is {float(decompose.resid.var())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.485578Z","iopub.execute_input":"2025-02-05T22:16:41.486735Z","iopub.status.idle":"2025-02-05T22:16:41.508503Z","shell.execute_reply.started":"2025-02-05T22:16:41.486673Z","shell.execute_reply":"2025-02-05T22:16:41.506354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def step_auto_arima(train, val):\n    smape_scorer = make_scorer(smape, greater_is_better=False)\n    mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n\n    model_arima = auto_arima(\n        train[['y']],\n        test='adf',\n        m=12,\n        seasonal_test='ocsb',\n        start_P=0,\n        seasonal=True,\n        d=None,\n        D=1,\n        trace=True,  \n        error_action='ignore',\n        suppress_warnings=True,\n        stepwise=True,\n        n_jobs=-1,\n        maxiter=1\n    )\n\n    preds = model_arima.predict(n_periods=len(val))\n    mape_score = mean_absolute_percentage_error(val['y'], preds)\n    print(f\"MAPE for time series score: {mape_score}\")\n\n    print(\"\\n\")\n    print(model_arima.summary())\n\n    fig = model_arima.plot_diagnostics()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Arima_Diagnostics_\",'png'))  \n    plt.close(fig)\n\n\n    plt.plot(val['ds'], val['y'], label='Actual')\n    plt.plot(val['ds'], preds, label='ARIMA Forecast')\n    plt.title('Actual vs. Forecasted Accidents')\n    plt.xlabel('Date')\n    plt.ylabel('Score')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Arima_Forecast_\",'png'))  \n    plt.close()  \n    \n    return model_arima","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.509924Z","iopub.execute_input":"2025-02-05T22:16:41.510344Z","iopub.status.idle":"2025-02-05T22:16:41.539562Z","shell.execute_reply.started":"2025-02-05T22:16:41.510294Z","shell.execute_reply":"2025-02-05T22:16:41.537235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ndef step_neural_prophet(df):\n    df = df[['ds','y','summer','winter','fall','spring','weekend','road_surface','weather','lum']]\n    set_log_level(\"ERROR\")\n\n    m = NeuralProphet(\n        n_changepoints=10,\n        seasonality_mode='multiplicative',\n        yearly_seasonality=True,\n        weekly_seasonality=True,\n        daily_seasonality=False,\n        n_lags=25\n    )\n    m.set_plotting_backend(\"matplotlib\")  \n\n    m = m.add_country_holidays(country_name=\"FR\")\n    m.add_lagged_regressor(\"road_surface\", n_lags=1)\n    m.add_future_regressor(name=\"weather\")\n    m.add_lagged_regressor(\"lum\", n_lags=1)\n\n    m.add_seasonality(name=\"summer\", period=7, fourier_order=14, condition_name=\"summer\")\n    m.add_seasonality(name=\"winter\", period=7, fourier_order=14, condition_name=\"winter\")\n    m.add_seasonality(name=\"spring\", period=7, fourier_order=14, condition_name=\"spring\")\n    m.add_seasonality(name=\"fall\", period=7, fourier_order=14, condition_name=\"fall\")\n    m.add_seasonality(name=\"weekend\", period=1, fourier_order=6, condition_name=\"weekend\")\n\n    df_train, df_test = m.split_df(df, valid_p=0.1, local_split=True)\n    print(f\"Train shape: {df_train.shape}\")\n    print(f\"Test shape: {df_test.shape}\")\n\n    metrics = m.fit(df_train, num_workers=3)\n    forecast = m.predict(df)\n\n    m.highlight_nth_step_ahead_of_each_forecast(1)\n    \n    fig = m.plot(forecast)\n    plt.savefig(ExtensionMethods.generate_filename(\"Neural_Prophet_Plot_\",'png'))  \n    plt.close(fig)\n    \n    fig = m.plot_components(forecast)\n    plt.savefig(ExtensionMethods.generate_filename(\"Neural_Prophet_Plot_Components\",'png'))  \n    plt.close(fig)\n    \n    fig = m.plot_parameters()\n    plt.savefig(ExtensionMethods.generate_filename(\"Neural_Prophet_Plot_Parameters\",'png'))  \n    plt.close(fig)\n\n\n    a = forecast.tail(len(df_test))\n    mape_score = mean_absolute_percentage_error(df_test['y'], a['yhat1'])\n    print(f\"MAPE for time series score: {mape_score}\")\n\n    fig, ax = plt.subplots(figsize=(20,20))\n    ax.plot(df_test['ds'], df_test['y'], label='Actual')\n    ax.plot(df_test['ds'], a['yhat1'], label='Prophet Forecast')\n    ax.legend()\n    plt.savefig(ExtensionMethods.generate_filename(\"Neural_Prophet_Actual_vs_Forecast\",'png'))\n    plt.close(fig) \n\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.540815Z","iopub.execute_input":"2025-02-05T22:16:41.541228Z","iopub.status.idle":"2025-02-05T22:16:41.572738Z","shell.execute_reply.started":"2025-02-05T22:16:41.541192Z","shell.execute_reply":"2025-02-05T22:16:41.571320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ndef step_prophet(train,val):\n    train = train[['ds','y']] # prophet doenst like anything else, thats why neuralprophet is a bit better.\n    val = val[['ds','y']]\n    train['year'] = train['ds'].dt.year\n    train['month'] = train['ds'].dt.month\n    train['day'] = train['ds'].dt.day\n\n    model = Prophet(changepoint_prior_scale=0.5,seasonality_mode='multiplicative') \n    model.add_country_holidays(country_name='FR')\n    model.fit(train)\n\n    future = model.make_future_dataframe(periods=len(val))\n    future['year'] = future['ds'].dt.year\n    future['month'] = future['ds'].dt.month\n    future['day'] = future['ds'].dt.day\n\n    forecast = model.predict(future)\n\n    fig = model.plot(forecast)\n    plt.title(\"Prophet Accident forecast\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Accidents\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Prophet_Accident_Forecast\",'png'))\n    plt.close(fig)\n\n    fig = model.plot_components(forecast)\n    plt.title(\"Prophet Model Future Components\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(ExtensionMethods.generate_filename(\"Prophet_Model_Future_Compoents\",'png'))\n    plt.close(fig)\n\n    a = forecast.tail(len(val))\n    mape_score = mean_absolute_percentage_error(val['y'], a['yhat'])\n    print(f\"MAPE for time series score: {mape_score}\")\n\n    fig, ax = plt.subplots(figsize=(20,20))\n    ax.plot(val['ds'], val['y'], label='Actual')\n    ax.plot(val['ds'], a['yhat'], label='Prophet Forecast')\n    ax.legend()\n    plt.savefig(ExtensionMethods.generate_filename(\"Prophet_Actual_vs_Forecast\",'png'))\n    plt.close(fig) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.573959Z","iopub.execute_input":"2025-02-05T22:16:41.574287Z","iopub.status.idle":"2025-02-05T22:16:41.605456Z","shell.execute_reply.started":"2025-02-05T22:16:41.574261Z","shell.execute_reply":"2025-02-05T22:16:41.604040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= step1()\ndata = step5(step4(step3(step2(df)))) ## stupid but its fun to do bad code :)\ntime_series_analyser(data)\n\ntrain,val = step6(data)\np_model = step_prophet(train,val)\n\narima_model = step_auto_arima(train,val)\n\nneural_model = step_neural_prophet(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:41.607310Z","iopub.execute_input":"2025-02-05T22:16:41.607730Z","iopub.status.idle":"2025-02-05T22:24:44.259297Z","shell.execute_reply.started":"2025-02-05T22:16:41.607694Z","shell.execute_reply":"2025-02-05T22:24:44.257400Z"}},"outputs":[],"execution_count":null}]}